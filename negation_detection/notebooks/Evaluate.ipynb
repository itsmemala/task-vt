{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "out_dir = '../data/output/'\n",
    "annotated_out_dir = '../data/output/annotated_output/'\n",
    "\n",
    "output_validation_data = pd.read_excel(data_dir + 'DrugVisData - All Annotations V2.xlsx',\\\n",
    "                                       sheet_name = 'DrugVisData - Copy')\n",
    "\n",
    "output_data_1 = pd.read_csv(out_dir + 'DrugVisData_Negated_Output.csv')\n",
    "output_data_2 = pd.read_excel(out_dir + 'DrugVisData_Negated_Output_2.xlsx', sheet_name='DrugVisData_Negated_Output_2')\n",
    "output_data_3 = pd.read_excel(out_dir + 'DrugVisData_Negated_Output_Parsed.xlsx', sheet_name='DrugVisData_Negated_Output_Pars')\n",
    "output_data_4 = pd.read_csv(out_dir + 'DrugVisdata_Negation_Output_Parsed_D.csv')\n",
    "output_data_5 = pd.read_csv(out_dir + 'DrugVisdata_Negation_Output_Negspacy.csv')\n",
    "output_data_6 = pd.read_csv(annotated_out_dir + 'compare_1_step.csv')\n",
    "output_data_7 = pd.read_csv(annotated_out_dir + 'compare_2_step.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_1 = output_data_1.join(output_validation_data[['Unnamed: 0','annotation_expert_1','Treatment']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_2 = output_data_2.join(output_validation_data[['Unnamed: 0','annotation_expert_1','Treatment']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_3 = output_data_3.join(output_validation_data[['Unnamed: 0','annotation_expert_1','Treatment']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_4 = output_data_4.join(output_validation_data[['Unnamed: 0','annotation_expert_1','Treatment']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_5 = output_data_5.join(output_validation_data[['Unnamed: 0','annotation_expert_1','Treatment']].set_index('Unnamed: 0'),\\\n",
    "                   on='Unnamed: 0', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_6 = output_data_6.join(output_validation_data[['sentence','annotation_expert_1','drug','Treatment']].set_index('sentence'),\\\n",
    "                   on='sentence', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])\n",
    "output_data_7 = output_data_7.join(output_validation_data[['sentence','annotation_expert_1','drug','Treatment']].set_index('sentence'),\\\n",
    "                   on='sentence', how = 'inner',\\\n",
    "                   lsuffix = '_out', rsuffix = '_val')\\\n",
    "                .dropna(subset=['annotation_expert_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_6['preds'] = [1 if p>=0.5 else 0 for p in output_data_6['preds']]\n",
    "output_data_7['preds'] = [1 if p>=0.5 else 0 for p in output_data_7['preds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_1.to_csv(annotated_out_dir + 'DrugVisData_Negated_Output.csv')\n",
    "output_data_2.to_csv(annotated_out_dir + 'DrugVisData_Negated_Output_2.csv')\n",
    "output_data_3.to_csv(annotated_out_dir + 'DrugVisData_Negated_Output_Parsed.csv')\n",
    "output_data_4.to_csv(annotated_out_dir + 'DrugVisdata_Negation_Output_Parsed_D.csv')\n",
    "output_data_5.to_csv(annotated_out_dir + 'DrugVisdata_Negation_Output_Negspacy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n",
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n",
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n",
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n",
      "0.0    337\n",
      "1.0     42\n",
      "Name: annotation_expert_1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(output_data_1['annotation_expert_1'].value_counts())\n",
    "print(output_data_2['annotation_expert_1'].value_counts())\n",
    "print(output_data_3['annotation_expert_1'].value_counts())\n",
    "print(output_data_4['annotation_expert_1'].value_counts())\n",
    "print(output_data_5['annotation_expert_1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    367\n",
       "True      12\n",
       "Name: Is_Negated_Parsed, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_3['Is_Negated_Parsed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for model using trigger terms V1 (using StanfordCoreNLP to tokenize):\n",
      "\n",
      "[[287  50]\n",
      " [ 30  12]]\n",
      "Confusion Matrix for model using trigger terms V1 (without StanfordCoreNLP to tokenize):\n",
      "\n",
      "[[287  50]\n",
      " [ 30  12]]\n",
      "Confusion Matrix for model using trigger terms and syntactic parsing to identify negated part\n",
      "Checks if negated part contains drug name (done in excel):\n",
      "\n",
      "[[325  12]\n",
      " [ 42   0]]\n",
      "Confusion Matrix for model using scispacy dependency parsing:\n",
      "\n",
      "[[321  16]\n",
      " [ 39   3]]\n",
      "Confusion Matrix for model using scispacy entity recognition and negspacy:\n",
      "\n",
      "[[317  20]\n",
      " [ 34   8]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for model using trigger terms V1 (using StanfordCoreNLP to tokenize):\\n')\n",
    "print(confusion_matrix(output_data_1['annotation_expert_1'], output_data_1['Is_Negated']))\n",
    "\n",
    "print('Confusion Matrix for model using trigger terms V1 (without StanfordCoreNLP to tokenize):\\n')\n",
    "print(confusion_matrix(output_data_2['annotation_expert_1'], output_data_2['Is_Negated']))\n",
    "\n",
    "print('Confusion Matrix for model using trigger terms and syntactic parsing to identify negated part\\nChecks if negated part contains drug name (done in excel):\\n')\n",
    "print(confusion_matrix(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed']))\n",
    "\n",
    "print('Confusion Matrix for model using scispacy dependency parsing:\\n')\n",
    "print(confusion_matrix(output_data_4['annotation_expert_1'], output_data_4['Is_Negated']))\n",
    "\n",
    "print('Confusion Matrix for model using scispacy entity recognition and negspacy:\\n')\n",
    "print(confusion_matrix(output_data_5['annotation_expert_1'], output_data_5['Is_Negated']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model using trigger terms V1 (using StanfordCoreNLP to tokenize):\n",
      "\n",
      "Overall accuracy: 0.7889182058047494\n",
      "Precision: 0.1935483870967742\n",
      "Recall: 0.2857142857142857\n",
      "F1 score: 0.23076923076923075\n",
      "\n",
      "\n",
      "Model using trigger terms V1 (without StanfordCoreNLP to tokenize):\n",
      "\n",
      "Overall accuracy: 0.7889182058047494\n",
      "Precision: 0.1935483870967742\n",
      "Recall: 0.2857142857142857\n",
      "F1 score: 0.23076923076923075\n",
      "\n",
      "\n",
      "Model using trigger terms and syntactic parsing to identify negated part\n",
      "Checks if negated part contains drug name (done in excel):\n",
      "\n",
      "Overall accuracy: 0.8575197889182058\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 score: 0.0\n",
      "\n",
      "\n",
      "Model using scispacy dependency parsing:\n",
      "\n",
      "Overall accuracy: 0.8548812664907651\n",
      "Precision: 0.15789473684210525\n",
      "Recall: 0.07142857142857142\n",
      "F1 score: 0.09836065573770492\n",
      "\n",
      "\n",
      "Model using scispacy entity recognition and negspacy:\n",
      "\n",
      "Overall accuracy: 0.8575197889182058\n",
      "Precision: 0.2857142857142857\n",
      "Recall: 0.19047619047619047\n",
      "F1 score: 0.22857142857142854\n"
     ]
    }
   ],
   "source": [
    "print('Model using trigger terms V1 (using StanfordCoreNLP to tokenize):\\n')\n",
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_1['annotation_expert_1'], output_data_1['Is_Negated'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_1['annotation_expert_1'], output_data_1['Is_Negated'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_1['annotation_expert_1'], output_data_1['Is_Negated'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_1['annotation_expert_1'], output_data_1['Is_Negated'] )))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Model using trigger terms V1 (without StanfordCoreNLP to tokenize):\\n')\n",
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_2['annotation_expert_1'], output_data_2['Is_Negated'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_2['annotation_expert_1'], output_data_2['Is_Negated'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_2['annotation_expert_1'], output_data_2['Is_Negated'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_2['annotation_expert_1'], output_data_2['Is_Negated'] )))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Model using trigger terms and syntactic parsing to identify negated part\\nChecks if negated part contains drug name (done in excel):\\n')\n",
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_3['annotation_expert_1'], output_data_3['Is_Negated_Parsed'] )))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Model using scispacy dependency parsing:\\n')\n",
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_4['annotation_expert_1'], output_data_4['Is_Negated'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_4['annotation_expert_1'], output_data_4['Is_Negated'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_4['annotation_expert_1'], output_data_4['Is_Negated'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_4['annotation_expert_1'], output_data_4['Is_Negated'] )))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Model using scispacy entity recognition and negspacy:\\n')\n",
    "print('Overall accuracy: '\\\n",
    "      + str(accuracy_score(output_data_5['annotation_expert_1'], output_data_5['Is_Negated'] )))\n",
    "print('Precision: '\\\n",
    "      + str(precision_score(output_data_5['annotation_expert_1'], output_data_5['Is_Negated'] )))\n",
    "print('Recall: '\\\n",
    "      + str(recall_score(output_data_5['annotation_expert_1'], output_data_5['Is_Negated'] )))\n",
    "print('F1 score: '\\\n",
    "      + str(f1_score(output_data_5['annotation_expert_1'], output_data_5['Is_Negated'] )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for patterns in mis-classified sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median length:198.0\n",
      "75% percentile length:303.0\n",
      "Max length:2292\n"
     ]
    }
   ],
   "source": [
    "print('\\nMedian length:'+str(output_data_1.sentence.str.len().median()))\n",
    "print('75% percentile length:'+str(output_data_1.sentence.str.len().quantile(.75)))\n",
    "print('Max length:'+str(output_data_1.sentence.str.len().max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model using trigger terms V1 (using StanfordCoreNLP to tokenize):\n",
      "\n",
      "Total misclassifications:\n",
      "                sentence\n",
      "Treatment               \n",
      "treated               15\n",
      "suppress              11\n",
      "treat                  7\n",
      "administered           6\n",
      "therapy                5\n",
      "administer             4\n",
      "treating               4\n",
      "administration         2\n",
      "effect                 2\n",
      "prescribed             2\n",
      "therapeutic            2\n",
      "treatment              2\n",
      "\n",
      "False positives:\n",
      "                sentence\n",
      "Treatment               \n",
      "treated               10\n",
      "administered           6\n",
      "treat                  3\n",
      "treating               3\n",
      "prescribed             2\n",
      "suppress               2\n",
      "therapeutic            2\n",
      "therapy                2\n",
      "administer             1\n",
      "administration         1\n",
      "treatment              1\n",
      "\n",
      "False negatives:\n",
      "                sentence\n",
      "Treatment               \n",
      "suppress               9\n",
      "treated                5\n",
      "treat                  4\n",
      "administer             3\n",
      "therapy                3\n",
      "effect                 2\n",
      "administration         1\n",
      "treating               1\n",
      "treatment              1\n",
      "\n",
      "Median length:261.0\n",
      "75% percentile length:305.0\n",
      "Max length:637\n"
     ]
    }
   ],
   "source": [
    "print('Model using trigger terms V1 (using StanfordCoreNLP to tokenize):\\n')\n",
    "print('Total misclassifications:')\n",
    "misclass_1 = output_data_1.loc[output_data_1.annotation_expert_1 != output_data_1.Is_Negated,['sentence','drug','Treatment','annotation_expert_1','Is_Negated']].drop_duplicates()\n",
    "print(misclass_1[['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nFalse positives:')\n",
    "print(misclass_1.loc[misclass_1.Is_Negated==1,['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nFalse negatives:')\n",
    "print(misclass_1.loc[misclass_1.Is_Negated==0,['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nMedian length:'+str(misclass_1.sentence.str.len().median()))\n",
    "print('75% percentile length:'+str(misclass_1.sentence.str.len().quantile(.75)))\n",
    "print('Max length:'+str(misclass_1.sentence.str.len().max()))\n",
    "\n",
    "#Remove cases with multiple sentences and check again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model using scispacy entity recognition and negspacy:\n",
      "\n",
      "Total misclassifications:\n",
      "                sentence\n",
      "Treatment               \n",
      "suppress              10\n",
      "treated                9\n",
      "treat                  5\n",
      "administered           4\n",
      "administer             3\n",
      "therapy                3\n",
      "treating               3\n",
      "administration         2\n",
      "effect                 2\n",
      "treatment              2\n",
      "effects                1\n",
      "prescribe              1\n",
      "supression             1\n",
      "\n",
      "False positives:\n",
      "                sentence\n",
      "Treatment               \n",
      "administered           4\n",
      "treated                4\n",
      "treating               2\n",
      "administration         1\n",
      "effects                1\n",
      "treatment              1\n",
      "\n",
      "False negatives:\n",
      "                sentence\n",
      "Treatment               \n",
      "suppress              10\n",
      "treat                  5\n",
      "treated                5\n",
      "administer             3\n",
      "therapy                3\n",
      "effect                 2\n",
      "administration         1\n",
      "prescribe              1\n",
      "supression             1\n",
      "treating               1\n",
      "treatment              1\n",
      "\n",
      "Median length:202.0\n",
      "75% percentile length:299.75\n",
      "Max length:637\n"
     ]
    }
   ],
   "source": [
    "print('Model using scispacy entity recognition and negspacy:\\n')\n",
    "print('Total misclassifications:')\n",
    "misclass_5 = output_data_5.loc[output_data_5.annotation_expert_1 != output_data_5.Is_Negated,['sentence','drug','Treatment','annotation_expert_1','Is_Negated']].drop_duplicates()\n",
    "print(misclass_5[['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nFalse positives:')\n",
    "print(misclass_5.loc[misclass_5.Is_Negated==1,['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nFalse negatives:')\n",
    "print(misclass_5.loc[misclass_5.Is_Negated==0,['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nMedian length:'+str(misclass_5.sentence.str.len().median()))\n",
    "print('75% percentile length:'+str(misclass_5.sentence.str.len().quantile(.75)))\n",
    "print('Max length:'+str(misclass_5.sentence.str.len().max()))\n",
    "\n",
    "#Remove cases with multiple sentences and check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model using Roberta Base fine tuned on Biocorpus abstracts:\n",
      "\n",
      "Total misclassifications:\n",
      "                sentence\n",
      "Treatment               \n",
      "treated               13\n",
      "suppress              11\n",
      "administered           6\n",
      "treat                  5\n",
      "administer             4\n",
      "therapy                3\n",
      "administration         2\n",
      "effect                 2\n",
      "prescribed             2\n",
      "therapeutic            2\n",
      "treating               2\n",
      "treatment              2\n",
      "\n",
      "False positives:\n",
      "                sentence\n",
      "Treatment               \n",
      "treated                8\n",
      "administered           6\n",
      "treat                  3\n",
      "prescribed             2\n",
      "suppress               2\n",
      "therapeutic            2\n",
      "treating               2\n",
      "administer             1\n",
      "administration         1\n",
      "treatment              1\n",
      "\n",
      "False negatives:\n",
      "                sentence\n",
      "Treatment               \n",
      "suppress               9\n",
      "treated                5\n",
      "administer             3\n",
      "therapy                3\n",
      "effect                 2\n",
      "treat                  2\n",
      "administration         1\n",
      "treatment              1\n",
      "\n",
      "Median length:260.0\n",
      "75% percentile length:305.0\n",
      "Max length:637\n"
     ]
    }
   ],
   "source": [
    "print('Model using Roberta Base fine tuned on Biocorpus abstracts:\\n')\n",
    "print('Total misclassifications:')\n",
    "misclass_6 = output_data_6.loc[output_data_6.annotation_expert_1 != output_data_6.preds,['sentence','drug','Treatment','annotation_expert_1','preds']].drop_duplicates()\n",
    "print(misclass_6[['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nFalse positives:')\n",
    "print(misclass_6.loc[misclass_6.preds==1,['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nFalse negatives:')\n",
    "print(misclass_6.loc[misclass_6.preds==0,['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nMedian length:'+str(misclass_6.sentence.str.len().median()))\n",
    "print('75% percentile length:'+str(misclass_6.sentence.str.len().quantile(.75)))\n",
    "print('Max length:'+str(misclass_6.sentence.str.len().max()))\n",
    "\n",
    "#Remove cases with multiple sentences and check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model using Roberta Base fine tuned on Biocorpus abstracts = annotated DrugVisData:\n",
      "\n",
      "Total misclassifications:\n",
      "                sentence\n",
      "Treatment               \n",
      "suppress              10\n",
      "treated                5\n",
      "administer             3\n",
      "therapy                3\n",
      "effect                 2\n",
      "treat                  2\n",
      "administration         1\n",
      "prescribe              1\n",
      "supression             1\n",
      "treatment              1\n",
      "\n",
      "False positives:\n",
      "Empty DataFrame\n",
      "Columns: [sentence]\n",
      "Index: []\n",
      "\n",
      "False negatives:\n",
      "                sentence\n",
      "Treatment               \n",
      "suppress              10\n",
      "treated                5\n",
      "administer             3\n",
      "therapy                3\n",
      "effect                 2\n",
      "treat                  2\n",
      "administration         1\n",
      "prescribe              1\n",
      "supression             1\n",
      "treatment              1\n",
      "\n",
      "Median length:195.0\n",
      "75% percentile length:291.0\n",
      "Max length:637\n"
     ]
    }
   ],
   "source": [
    "print('Model using Roberta Base fine tuned on Biocorpus abstracts = annotated DrugVisData:\\n')\n",
    "print('Total misclassifications:')\n",
    "misclass_7 = output_data_7.loc[output_data_7.annotation_expert_1 != output_data_7.preds,['sentence','drug','Treatment','annotation_expert_1','preds']].drop_duplicates()\n",
    "print(misclass_7[['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nFalse positives:')\n",
    "print(misclass_7.loc[misclass_7.preds==1,['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nFalse negatives:')\n",
    "print(misclass_7.loc[misclass_7.preds==0,['Treatment','sentence']].groupby('Treatment').count().sort_values(by='sentence',ascending=False))\n",
    "\n",
    "print('\\nMedian length:'+str(misclass_7.sentence.str.len().median()))\n",
    "print('75% percentile length:'+str(misclass_7.sentence.str.len().quantile(.75)))\n",
    "print('Max length:'+str(misclass_7.sentence.str.len().max()))\n",
    "\n",
    "#Remove cases with multiple sentences and check again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
